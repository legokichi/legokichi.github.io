
<h1>
<span id="theano-の-opencl-バックエンド-を-core-i5-haswell-の-ubuntu1404-上で実行する" class="fragment"></span><a href="#theano-%E3%81%AE-opencl-%E3%83%90%E3%83%83%E3%82%AF%E3%82%A8%E3%83%B3%E3%83%89-%E3%82%92-core-i5-haswell-%E3%81%AE-ubuntu1404-%E4%B8%8A%E3%81%A7%E5%AE%9F%E8%A1%8C%E3%81%99%E3%82%8B"><i class="fa fa-link"></i></a>Theano の OpenCL バックエンド を Core i5 Haswell の Ubuntu14.04 上で実行する</h1>

<p>先行例: <a href="http://onlybrain.exblog.jp/25776518/" class="autolink" rel="nofollow noopener" target="_blank">http://onlybrain.exblog.jp/25776518/</a></p>

<p>※ この記事では <code>pyenv shell anaconda3-4.3.0</code> を使用しています。</p>

<h2>
<span id="beignet-をインストールする" class="fragment"></span><a href="#beignet-%E3%82%92%E3%82%A4%E3%83%B3%E3%82%B9%E3%83%88%E3%83%BC%E3%83%AB%E3%81%99%E3%82%8B"><i class="fa fa-link"></i></a>Beignet をインストールする</h2>

<p>haswell 世代では Intel 公式の OpenCL GPU のサポートがないので Beignet を使います。Beignet でも Haswell 世代では Linux Kernel 4.2 以下ではカーネルにパッチを当てる必要があります。<code>uname -r</code> でカーネルバージョンを確認しておきましょう。</p>

<ul>
<li>intel opencl の対応状況はここ - <a href="https://software.intel.com/en-us/intel-opencl" class="autolink" rel="nofollow noopener" target="_blank">https://software.intel.com/en-us/intel-opencl</a>
</li>
<li>その他のハードウェア環境 + Linux 上での GPGPU の手段については ArchLinux Wiki が明るいです - <a href="https://wiki.archlinuxjp.org/index.php/GPGPU" class="autolink" rel="nofollow noopener" target="_blank">https://wiki.archlinuxjp.org/index.php/GPGPU</a>
</li>
</ul>

<p>インストールには</p>

<div class="code-frame" data-lang="sh"><div class="highlight"><pre><span class="nb">sudo </span>apt-get <span class="nb">install </span>beignet-opencl-icd
</pre></div></div>

<p>するか、もしくは <a href="https://www.freedesktop.org/wiki/Software/Beignet/" class="autolink" rel="nofollow noopener" target="_blank">https://www.freedesktop.org/wiki/Software/Beignet/</a> を参考にソースからビルドします。</p>

<div class="code-frame" data-lang="sh"><div class="highlight"><pre><span class="nb">sudo </span>apt-get <span class="nb">install </span>cmake pkg-config python ocl-icd-dev libegl1-mesa-dev ocl-icd-opencl-dev libdrm-dev libxfixes-dev libxext-dev llvm-3.6-dev clang-3.6 libclang-3.6-dev libtinfo-dev libedit-dev zlib1g-dev
git clone https://anongit.freedesktop.org/git/beignet.git
<span class="nb">cd </span>beignet
<span class="nb">mkdir </span>build
<span class="nb">cd </span>build
cmake .. <span class="nt">-DCMAKE_BUILD_TYPE</span><span class="o">=</span>Release
make
<span class="nb">sudo </span>make <span class="nb">install</span>
</pre></div></div>

<p>ユニットテストしたい場合は</p>

<div class="code-frame" data-lang="sh"><div class="highlight"><pre>make utest
<span class="nb">cd </span>utests
<span class="nb">source </span>setenv.sh
./utest_run
</pre></div></div>

<p>とすればできます。</p>

<h3>
<span id="clinfo-コマンドで-opencl-環境を確認する" class="fragment"></span><a href="#clinfo-%E3%82%B3%E3%83%9E%E3%83%B3%E3%83%89%E3%81%A7-opencl-%E7%92%B0%E5%A2%83%E3%82%92%E7%A2%BA%E8%AA%8D%E3%81%99%E3%82%8B"><i class="fa fa-link"></i></a>clinfo コマンドで OpenCL 環境を確認する</h3>

<div class="code-frame" data-lang="sh"><div class="highlight"><pre><span class="nb">sudo </span>apt-get <span class="nb">install </span>clinfo
</pre></div></div>

<div class="code-frame" data-lang="sh"><div class="highlight"><pre><span class="nv">$ </span>clinfo
Number of platforms                               1
  Platform Name                                   Intel Gen OCL Driver
  Platform Vendor                                 Intel
  Platform Version                                OpenCL 1.2 beignet 1.4 <span class="o">(</span>git-beaf26f<span class="o">)</span>
  Platform Profile                                FULL_PROFILE
  Platform Extensions                             cl_khr_global_int32_base_atomics cl_khr_global_int32_extended_atomics cl_khr_local_int32_base_atomics cl_khr_local_int32_extended_atomics cl_khr_byte_addressable_store cl_khr_3d_image_writes cl_khr_image2d_from_buffer cl_khr_depth_images cl_khr_spir cl_khr_icd cl_intel_accelerator cl_intel_subgroups cl_intel_subgroups_short cl_intel_media_block_io cl_intel_planar_yuv
  Platform Extensions <span class="k">function </span>suffix             Intel
Beignet: self-test failed: <span class="o">(</span>3, 7, 5<span class="o">)</span> + <span class="o">(</span>5, 7, 3<span class="o">)</span> returned <span class="o">(</span>3, 7, 5<span class="o">)</span>
See README.md or http://www.freedesktop.org/wiki/Software/Beignet/
Beignet: Warning - overriding self-test failure

  Platform Name                                   Intel Gen OCL Driver
Number of devices                                 1
Beignet: Warning - overriding self-test failure
  Device Name                                     Intel<span class="o">(</span>R<span class="o">)</span> HD Graphics Haswell Ultrabook GT2 Mobile
  Device Vendor                                   Intel
  Device Vendor ID                                0x8086
  Device Version                                  OpenCL 1.2 beignet 1.4 <span class="o">(</span>git-beaf26f<span class="o">)</span>
  Driver Version                                  1.4
  Device OpenCL C Version                         OpenCL C 1.2 beignet 1.4 <span class="o">(</span>git-beaf26f<span class="o">)</span>
  Device Type                                     GPU
  Device Profile                                  FULL_PROFILE
  Max compute units                               20
  Max clock frequency                             1000MHz
  Device Partition                                <span class="o">(</span>core<span class="o">)</span>
    Max number of sub-devices                     1
    Supported partition types                     None, None, None
  Max work item dimensions                        3
  Max work item sizes                             512x512x512
  Max work group size                             512
  Preferred work group size multiple              16
  Preferred / native vector sizes                 
    char                                                16 / 8       
    short                                                8 / 8       
    int                                                  4 / 4       
    long                                                 2 / 2       
    half                                                 0 / 8        <span class="o">(</span>n/a<span class="o">)</span>
    float                                                4 / 4       
    double                                               0 / 2        <span class="o">(</span>n/a<span class="o">)</span>
  Half-precision Floating-point support           <span class="o">(</span>n/a<span class="o">)</span>
  Single-precision Floating-point support         <span class="o">(</span>core<span class="o">)</span>
    Denormals                                     No
    Infinity and NANs                             Yes
    Round to nearest                              Yes
    Round to zero                                 No
    Round to infinity                             No
    IEEE754-2008 fused multiply-add               No
    Support is emulated <span class="k">in </span>software               No
    Correctly-rounded divide and sqrt operations  No
  Double-precision Floating-point support         <span class="o">(</span>n/a<span class="o">)</span>
  Address bits                                    32, Little-Endian
  Global memory size                              2021654528 <span class="o">(</span>1.883GiB<span class="o">)</span>
  Error Correction support                        No
  Max memory allocation                           1516240896 <span class="o">(</span>1.412GiB<span class="o">)</span>
  Unified memory <span class="k">for </span>Host and Device              Yes
  Minimum alignment <span class="k">for </span>any data <span class="nb">type             </span>128 bytes
  Alignment of base address                       1024 bits <span class="o">(</span>128 bytes<span class="o">)</span>
  Global Memory cache <span class="nb">type                        </span>Read/Write
  Global Memory cache size                        8192
  Global Memory cache line                        64 bytes
  Image support                                   Yes
    Max number of samplers per kernel             16
    Max size <span class="k">for </span>1D images from buffer            65536 pixels
    Max 1D or 2D image array size                 2048 images
    Base address alignment <span class="k">for </span>2D image buffers   4096 bytes
    Pitch alignment <span class="k">for </span>2D image buffers          1 bytes
    Max 2D image size                             8192x8192 pixels
    Max 3D image size                             8192x8192x2048 pixels
    Max number of <span class="nb">read </span>image args                 128
    Max number of write image args                8
  Local memory <span class="nb">type                               </span>Local
  Local memory size                               65536 <span class="o">(</span>64KiB<span class="o">)</span>
  Max constant buffer size                        134217728 <span class="o">(</span>128MiB<span class="o">)</span>
  Max number of constant args                     8
  Max size of kernel argument                     1024
  Queue properties                                
    Out-of-order execution                        No
    Profiling                                     Yes
  Prefer user <span class="nb">sync </span><span class="k">for </span>interop                    Yes
  Profiling timer resolution                      80ns
  Execution capabilities                          
    Run OpenCL kernels                            Yes
    Run native kernels                            Yes
    SPIR versions                                 1.2
  <span class="nb">printf</span><span class="o">()</span> buffer size                            1048576 <span class="o">(</span>1024KiB<span class="o">)</span>
  Built-in kernels                                __cl_copy_region_align4<span class="p">;</span>__cl_copy_region_align16<span class="p">;</span>__cl_cpy_region_unalign_same_offset<span class="p">;</span>__cl_copy_region_unalign_dst_offset<span class="p">;</span>__cl_copy_region_unalign_src_offset<span class="p">;</span>__cl_copy_buffer_rect<span class="p">;</span>__cl_copy_image_1d_to_1d<span class="p">;</span>__cl_copy_image_2d_to_2d<span class="p">;</span>__cl_copy_image_3d_to_2d<span class="p">;</span>__cl_copy_image_2d_to_3d<span class="p">;</span>__cl_copy_image_3d_to_3d<span class="p">;</span>__cl_copy_image_2d_to_buffer<span class="p">;</span>__cl_copy_image_3d_to_buffer<span class="p">;</span>__cl_copy_buffer_to_image_2d<span class="p">;</span>__cl_copy_buffer_to_image_3d<span class="p">;</span>__cl_fill_region_unalign<span class="p">;</span>__cl_fill_region_align2<span class="p">;</span>__cl_fill_region_align4<span class="p">;</span>__cl_fill_region_align8_2<span class="p">;</span>__cl_fill_region_align8_4<span class="p">;</span>__cl_fill_region_align8_8<span class="p">;</span>__cl_fill_region_align8_16<span class="p">;</span>__cl_fill_region_align128<span class="p">;</span>__cl_fill_image_1d<span class="p">;</span>__cl_fill_image_1d_array<span class="p">;</span>__cl_fill_image_2d<span class="p">;</span>__cl_fill_image_2d_array<span class="p">;</span>__cl_fill_image_3d<span class="p">;</span>
  Device Available                                Yes
  Compiler Available                              Yes
  Linker Available                                Yes
  Device Extensions                               cl_khr_global_int32_base_atomics cl_khr_global_int32_extended_atomics cl_khr_local_int32_base_atomics cl_khr_local_int32_extended_atomics cl_khr_byte_addressable_store cl_khr_3d_image_writes cl_khr_image2d_from_buffer cl_khr_depth_images cl_khr_spir cl_khr_icd cl_intel_accelerator cl_intel_subgroups cl_intel_subgroups_short cl_intel_media_block_io cl_intel_planar_yuv

NULL platform behavior
  clGetPlatformInfo<span class="o">(</span>NULL, CL_PLATFORM_NAME, ...<span class="o">)</span>  No platform
  clGetDeviceIDs<span class="o">(</span>NULL, CL_DEVICE_TYPE_ALL, ...<span class="o">)</span>   No platform
  clCreateContext<span class="o">(</span>NULL, ...<span class="o">)</span> <span class="o">[</span>default]            No platform
  clCreateContext<span class="o">(</span>NULL, ...<span class="o">)</span> <span class="o">[</span>other]              Success <span class="o">[</span>Intel]
  clCreateContextFromType<span class="o">(</span>NULL, CL_DEVICE_TYPE_CPU<span class="o">)</span>  No platform
  clCreateContextFromType<span class="o">(</span>NULL, CL_DEVICE_TYPE_GPU<span class="o">)</span>  No platform
  clCreateContextFromType<span class="o">(</span>NULL, CL_DEVICE_TYPE_ACCELERATOR<span class="o">)</span>  No platform
  clCreateContextFromType<span class="o">(</span>NULL, CL_DEVICE_TYPE_CUSTOM<span class="o">)</span>  No platform
  clCreateContextFromType<span class="o">(</span>NULL, CL_DEVICE_TYPE_ALL<span class="o">)</span>  No platform
</pre></div></div>

<p><code>source setenv.sh</code> をして環境変数が適切に設定されていればこのようにズラズラと情報が表示されます。</p>

<h2>
<span id="clblas-か-clblast-をインストールする" class="fragment"></span><a href="#clblas-%E3%81%8B-clblast-%E3%82%92%E3%82%A4%E3%83%B3%E3%82%B9%E3%83%88%E3%83%BC%E3%83%AB%E3%81%99%E3%82%8B"><i class="fa fa-link"></i></a>clBLAS か CLBlast をインストールする。</h2>

<p>Theano のバックエンドである libgpuarray を OpenCL 環境で動かすためには clBLAS　か CLBlast をインストールする必要があります。 clBlas と CLBlast の違いですが、 CLBlast のほうが特定の環境では高速なようです。両方入れた場合　CLBlast のほうが優先される？ようです。</p>

<ul>
<li><a href="https://github.com/clMathLibraries/clBLAS" class="autolink" rel="nofollow noopener" target="_blank">https://github.com/clMathLibraries/clBLAS</a></li>
<li><a href="https://github.com/CNugteren/CLBlast" class="autolink" rel="nofollow noopener" target="_blank">https://github.com/CNugteren/CLBlast</a></li>
</ul>

<h3>
<span id="clblasの場合" class="fragment"></span><a href="#clblas%E3%81%AE%E5%A0%B4%E5%90%88"><i class="fa fa-link"></i></a>clBLAS　の場合</h3>

<div class="code-frame" data-lang="sh"><div class="highlight"><pre>git clone https://github.com/clMathLibraries/clBLAS.git
<span class="nb">cd </span>clBLAS
<span class="nb">mkdir </span>build
<span class="nb">cd </span>build
cmake .. <span class="nt">-DCMAKE_BUILD_TYPE</span><span class="o">=</span>Release
make
<span class="nb">sudo </span>make <span class="nb">install</span>
</pre></div></div>

<h3>
<span id="clblast-の場合" class="fragment"></span><a href="#clblast-%E3%81%AE%E5%A0%B4%E5%90%88"><i class="fa fa-link"></i></a>CLBlast の場合</h3>

<div class="code-frame" data-lang="sh"><div class="highlight"><pre>git clone https://github.com/CNugteren/CLBlast.git
<span class="nb">cd </span>CLBlast
<span class="nb">mkdir </span>build
<span class="nb">cd </span>build
cmake .. <span class="nt">-DCMAKE_BUILD_TYPE</span><span class="o">=</span>Release
make
<span class="nb">sudo </span>make <span class="nb">install</span>
</pre></div></div>

<h2>
<span id="libgpuarray-と-pygpu-をインストールする" class="fragment"></span><a href="#libgpuarray-%E3%81%A8-pygpu-%E3%82%92%E3%82%A4%E3%83%B3%E3%82%B9%E3%83%88%E3%83%BC%E3%83%AB%E3%81%99%E3%82%8B"><i class="fa fa-link"></i></a>libgpuarray と pygpu をインストールする</h2>

<p>Theano のバックエンドである libgpuarray および pygpu をインストールします。conda や pip からではなくソースからのビルドが必要です。</p>

<div class="code-frame" data-lang="sh"><div class="highlight"><pre>git clone https://github.com/Theano/libgpuarray.git
<span class="nb">cd </span>libgpuarray
<span class="nb">mkdir </span>build
<span class="nb">cd </span>build
cmake .. <span class="nt">-DCMAKE_BUILD_TYPE</span><span class="o">=</span>Release
make
<span class="nb">sudo </span>make <span class="nb">install
cd</span> ..
python setup.py build
python setup.py <span class="nb">install</span>
</pre></div></div>

<h3>
<span id="pygpu-をテストする" class="fragment"></span><a href="#pygpu-%E3%82%92%E3%83%86%E3%82%B9%E3%83%88%E3%81%99%E3%82%8B"><i class="fa fa-link"></i></a>pygpu をテストする</h3>

<p>pygpu が opencl を認識できているかどうかを調べます。</p>

<div class="code-frame" data-lang="sh"><div class="highlight"><pre>python <span class="nt">-c</span> <span class="s2">"import pygpu; print(pygpu.init('opencl0:0').devname)"</span>
python <span class="nt">-c</span> <span class="s2">"import pygpu; print(pygpu.init('opencl0:1').devname)"</span>
python <span class="nt">-c</span> <span class="s2">"import pygpu; print(pygpu.init('opencl0:2').devname)"</span>
</pre></div></div>

<p><code>ValueError: No device 1</code> と表示されなければ OK です。<br>
また、 <code>Begnet: self-test failed</code> と表示されてエラーが出た場合は、 </p>

<div class="code-frame" data-lang="sh"><div class="highlight"><pre><span class="nb">export </span><span class="nv">OCL_IGNORE_SELF_TEST</span><span class="o">=</span>1
</pre></div></div>

<p>とすれば少々不安ですが、beignetの実行前テストを無視して実行できるようになります。自分の環境下では <code>Intel(R) HD Graphics Haswell Ultrabook GT2 Mobile</code> と表示されます。</p>

<p>pygpu のセルフテストをします。</p>

<div class="code-frame" data-lang="sh"><div class="highlight"><pre><span class="nv">DEVICE</span><span class="o">=</span><span class="s2">"opencl0:0"</span> python <span class="nt">-c</span> <span class="s2">"import pygpu;pygpu.test()"</span>
</pre></div></div>

<p><code>.</code> が沢山表示されれば成功ですが　<code>E</code> もたくさん表示されるかもしれません。</p>

<p><code>LoadError: ***.so.*: cannot open shared object file:  No such file or directory</code> とされた場合は共有ライブラリの情報が更新されていないかもしれないので </p>

<div class="code-frame" data-lang="sh"><div class="highlight"><pre><span class="nb">sudo </span>ldconfig
</pre></div></div>

<p>として依存関係情報を更新してください。</p>

<h2>
<span id="theano-をインストールする" class="fragment"></span><a href="#theano-%E3%82%92%E3%82%A4%E3%83%B3%E3%82%B9%E3%83%88%E3%83%BC%E3%83%AB%E3%81%99%E3%82%8B"><i class="fa fa-link"></i></a>Theano をインストールする</h2>

<p>pygpu が手動インストールできていれば theano は pip で十分です</p>

<div class="code-frame" data-lang="sh"><div class="highlight"><pre>pip <span class="nb">install </span>theano
</pre></div></div>

<h3>
<span id="theano-をテストする" class="fragment"></span><a href="#theano-%E3%82%92%E3%83%86%E3%82%B9%E3%83%88%E3%81%99%E3%82%8B"><i class="fa fa-link"></i></a>Theano をテストする</h3>

<p><a href="http://deeplearning.net/software/theano/tutorial/using_gpu.html" class="autolink" rel="nofollow noopener" target="_blank">http://deeplearning.net/software/theano/tutorial/using_gpu.html</a> にある次のスクリプトを実行することで theano が GPU で動いているかどうかを確認することができます。</p>

<div class="code-frame" data-lang="python">
<div class="code-lang"><span class="bold">theano_gpu_check.py</span></div>
<div class="highlight"><pre><span class="kn">from</span> <span class="nn">theano</span> <span class="kn">import</span> <span class="n">function</span><span class="p">,</span> <span class="n">config</span><span class="p">,</span> <span class="n">shared</span><span class="p">,</span> <span class="n">tensor</span>
<span class="kn">import</span> <span class="nn">numpy</span>
<span class="kn">import</span> <span class="nn">time</span>

<span class="n">vlen</span> <span class="o">=</span> <span class="mi">10</span> <span class="o">*</span> <span class="mi">30</span> <span class="o">*</span> <span class="mi">768</span>  <span class="c1"># 10 x #cores x # threads per core
</span><span class="n">iters</span> <span class="o">=</span> <span class="mi">1000</span>

<span class="n">rng</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">RandomState</span><span class="p">(</span><span class="mi">22</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">shared</span><span class="p">(</span><span class="n">numpy</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">rng</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">vlen</span><span class="p">),</span> <span class="n">config</span><span class="o">.</span><span class="n">floatX</span><span class="p">))</span>
<span class="n">f</span> <span class="o">=</span> <span class="n">function</span><span class="p">([],</span> <span class="n">tensor</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
<span class="k">print</span><span class="p">(</span><span class="n">f</span><span class="o">.</span><span class="n">maker</span><span class="o">.</span><span class="n">fgraph</span><span class="o">.</span><span class="n">toposort</span><span class="p">())</span>
<span class="n">t0</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">iters</span><span class="p">):</span>
    <span class="n">r</span> <span class="o">=</span> <span class="n">f</span><span class="p">()</span>
<span class="n">t1</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
<span class="k">print</span><span class="p">(</span><span class="s">"Looping </span><span class="si">%</span><span class="s">d times took </span><span class="si">%</span><span class="s">f seconds"</span> <span class="o">%</span> <span class="p">(</span><span class="n">iters</span><span class="p">,</span> <span class="n">t1</span> <span class="o">-</span> <span class="n">t0</span><span class="p">))</span>
<span class="k">print</span><span class="p">(</span><span class="s">"Result is </span><span class="si">%</span><span class="s">s"</span> <span class="o">%</span> <span class="p">(</span><span class="n">r</span><span class="p">,))</span>
<span class="k">if</span> <span class="n">numpy</span><span class="o">.</span><span class="nb">any</span><span class="p">([</span><span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">op</span><span class="p">,</span> <span class="n">tensor</span><span class="o">.</span><span class="n">Elemwise</span><span class="p">)</span> <span class="ow">and</span>
              <span class="p">(</span><span class="s">'Gpu'</span> <span class="ow">not</span> <span class="ow">in</span> <span class="nb">type</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">op</span><span class="p">)</span><span class="o">.</span><span class="n">__name__</span><span class="p">)</span>
              <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">f</span><span class="o">.</span><span class="n">maker</span><span class="o">.</span><span class="n">fgraph</span><span class="o">.</span><span class="n">toposort</span><span class="p">()]):</span>
    <span class="k">print</span><span class="p">(</span><span class="s">'Used the cpu'</span><span class="p">)</span>
<span class="k">else</span><span class="p">:</span>
    <span class="k">print</span><span class="p">(</span><span class="s">'Used the gpu'</span><span class="p">)</span>
</pre></div>
</div>

<p>フラグをつけて実行します。まず CPU で試してみます。</p>

<div class="code-frame" data-lang="sh"><div class="highlight"><pre><span class="nv">$ THEANO_FLAGS</span><span class="o">=</span><span class="nv">mode</span><span class="o">=</span>FAST_RUN,device<span class="o">=</span>cpu,floatX<span class="o">=</span>float32 python ./theano_gpu_check.py
<span class="o">[</span>Elemwise<span class="o">{</span>exp,no_inplace<span class="o">}(</span>&lt;TensorType<span class="o">(</span>float32, vector<span class="o">)&gt;)]</span>
Looping 1000 <span class="nb">times </span>took 34.545774 seconds
Result is <span class="o">[</span> 1.23178029  1.61879337  1.52278066 ...,  2.20771813  2.29967761
  1.62323284]
Used the cpu
</pre></div></div>

<p>次に opencl で試してみます。</p>

<div class="code-frame" data-lang="sh"><div class="highlight"><pre><span class="nv">$ THEANO_FLAGS</span><span class="o">=</span><span class="nv">mode</span><span class="o">=</span>FAST_RUN,device<span class="o">=</span>opencl0:0,floatX<span class="o">=</span>float32 python ./theano_gpu_check.py
Beignet: self-test failed: <span class="o">(</span>3, 7, 5<span class="o">)</span> + <span class="o">(</span>5, 7, 3<span class="o">)</span> returned <span class="o">(</span>3, 7, 5<span class="o">)</span>
See README.md or http://www.freedesktop.org/wiki/Software/Beignet/
Beignet: Warning - overriding self-test failure
Beignet: Warning - overriding self-test failure
Mapped name None to device opencl0:0: Intel<span class="o">(</span>R<span class="o">)</span> HD Graphics Haswell Ultrabook GT2 Mobile 
<span class="o">[</span>GpuElemwise<span class="o">{</span>exp,no_inplace<span class="o">}(</span>&lt;GpuArrayType&lt;None&gt;<span class="o">(</span>float32, <span class="o">(</span>False,<span class="o">))&gt;)</span>, HostFromGpu<span class="o">(</span>gpuarray<span class="o">)(</span>GpuElemwise<span class="o">{</span>exp,no_inplace<span class="o">}</span>.0<span class="o">)]</span>
Looping 1000 <span class="nb">times </span>took 1.114583 seconds
Result is <span class="o">[</span> 1.23178029  1.61879337  1.52278054 ...,  2.20771813  2.29967737
  1.62323284]
Used the gpu
</pre></div></div>

<p>34 秒かかっていた処理が 1 秒まで短縮されました。</p>

<p>しかしよく見るとベクトルの足し算のテストがおかしな結果になっていたり、CPUと結果が異なっていたりと怪しい挙動をしています。</p>

<p>また Theano や libgpuarray の issue では<a href="https://github.com/Theano/libgpuarray/issues/402#issuecomment-293387472" rel="nofollow noopener" target="_blank">こと</a>ある<a href="https://github.com/Theano/Theano/issues/5418#issuecomment-273523987" rel="nofollow noopener" target="_blank">ごと</a>に 「OpenCL サポートは実験的でありメンテする時間がない」と述べられています。</p>
